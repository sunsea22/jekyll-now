---
layout: post
title: 大数据面试题整理
---

bigdata面试


   面试机器学习、大数据岗位时遇到的各种问题

### 无监督和有监督算法的区别
监督学习：通过已有的训练样本（即已知数据以及其对应的输出）来训练，从而得到一个最优模型，再利用这个模型将所有新的数据样本映射为相应的输出结果，对输出结果进行简单的判断从而实现分类的目的，那么这个最优模型也就具有了对未知数据进行分类的能力。

无监督学习：我们事先没有任何训练数据样本，需要直接对数据进行建模。


### 逻辑斯蒂回归（LR）VS 决策树 VS 随机森林 VS SVM
LR与SVM

#### 不同
1. logistic regression适合需要得到一个分类概率的场景，SVM则则没有分类概率
2. LR其实同样可以使用kernel，但是LR没有support vector在计算复杂度上会高出很多。如果样本量很大并且需要的是一个复杂模型，那么建议SVM
3. 如果样本比较少，模型有比较复杂。那么建议SVM，它有一套比较好的解构风险最小化理论的保障，比如large margin和soft margin

#### 相同
1. 由于hinge loss和entropy loss很接近，因此得出来的两个分类面是非常接近的
2. 都是在两个loss上做了一个regularization

#### 逻辑回归
优点：
1.适合需要得到一个分类概率的场景

2.实现效率较高

3.对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决；

4.逻辑回归广泛的应用于工业问题上
 
缺点：
1.当特征空间很大时，逻辑回归的性能不是很好；

2.不能很好地处理大量多类特征或变量；

4.对于非线性特征，需要进行转换；

5.依赖于全部的数据特征，当特征有缺失的时候表现效果不好

#### 决策树
与决策树相关的最大问题是它们属于高度偏见型模型。你可以在训练集上构建决策树模型，而且其在训练集上的结果可能优于其它算法，但你的测试集最终会证明它是一个差的预测器。你必须对树进行剪枝，同时结合交叉验证才能得到一个没有过拟合的决策树模型。

#### 随机森林
随机森林在很大程度上克服了过拟合这一缺陷，其本身并没有什么特别之处，但它却是决策树一个非常优秀的扩展。

优点：
1.直观的决策规则

2.可以处理非线性特征

3.考虑了变量之间的相互作用

缺点：
1.训练集上的效果高度优于测试集，即过拟合[随机森林克服了此缺点]

2.没有将排名分数作为直接结果

#### SVM
支持向量机的特点是它依靠边界样本来建立需要的分离曲线，它可以处理非线性决策边界。支持向量机能够处理大的特征空间，也因此成为文本分析中最受欢迎的算法之一，由于文本数据几乎总是产生大量的特征，所以在这种情况下逻辑回归并不是一个非常好的选择。

优点：
1.能够处理大型特征空间

2.能够处理非线性特征之间的相互作用

3.无需依赖整个数据

缺点：
1.当观测样本很多时，效率并不是很高

2.有时候很难找到一个合适的核函数


### 为什么会产生过拟合，有哪些方法可以预防或克服过拟合

#### 什么是过拟合
所谓过拟合，是这样一个现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。

#### 过拟合产生的原因
出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少

#### 预防或克服措施
1 增大数据量
2 减少feature个数
3 正则化（留下所有的feature，但对于部分feature定义其parameter非常小）
4 交叉验证


### 聚类分析中的距离度量
1. 欧式距离：可以简单描述为多维空间的点点之间的几何距离
2. 曼哈顿距离：计算从一个对象到另一个对象所经过的折线距离
3. 切比雪夫距离：切比雪夫距离主要表现为在多维空间中，对象从某个位置转移到另外一个对象所消耗的最少距离
4. 幂距离：可以简单的描述为针对不同的属性给予不同的权重值


### Hadoop与Spark的特点及应用场景
Hadoop是离线计算，基于磁盘，每次运算之后的结果需要存储在HDFS里面，下次再用的话，还需要读出来进行一次计算，磁盘IO开销比较大。底层基于HDFS存储文件系统。并且Hadoop只有Map和Reduce两种接口，相对于Spark来说太少了。

Spark里面一个核心的概念就是RDD，弹性分布式数据集。Spark支持内存计算模型，用户可以指定存储策略，当内存不够的时候，可以放置到磁盘上。并且Spark提供了一组RDD的接口，Transformation和Action。Transformation是把一个RDD转换成为另一个RDD以便形成Lineage血统链，这样当数据发生错误的时候可以快速的依靠这种继承关系恢复数据。Action操作是启动一个job并开始真正的进行一些计算并把返回的结果可以给Driver或者是缓存在worker里面。


### 简单说一下hadoop和spark的shuffle过程
hadoop：map端保存分片数据，通过网络收集到reduce端
spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor

### Hbase行键列族的概念，物理模型，表的设计原则？
行健：是Hbase表自带的，每个行健对应一条数据
列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中的数据可以有很多，通过时间戳来区分
物理模型：整个Hbase表会拆分为多个region，每个region记录着行健的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点

### 简单说一下hadoop的map-reduce编程模型
首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合
将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value再输出
之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则
只会会对key进行sort排序，grouping分组操作将相同key的value合并分组输出
在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则
之后进行一个combine归约操作，其实就是一个本地段的reduce预处理，以减小后面shuffle和reducer的工作量
reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job

### Spark面对OOM问题的解决方法及优化总结

Spark中的OOM问题不外乎以下两种情况

1. map执行中内存溢出
2. shuffle后内存溢出


#### Spark内存模型

Spark在一个Executor中的内存分为三块，一块是execution内存，一块是storage内存，一块是other内存。

- execution内存是执行内存

- storage内存是存储broadcast，cache，persist数据的地方

- other内存是程序执行时预留给自己的内存。

  OOM的问题通常出现在execution这块内存中，因为storage这块内存在存放数据满了之后，会直接丢弃内存中旧的数据，对性能有影响但是不会有OOM的问题。

#### 内存溢出解决方法
1. map过程中产生大量对象导致内存溢出：
这种溢出的原因是在单个map中产生了大量的对象导致的。
针对这种问题，在不增加内存的情况下，具体做法可以在会产生大量对象的map操作之前调用reparation方法，分区成更小的块传入map。
2. 数据不平衡导致内存溢出：
数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面类似，就是调用reparation重新分区。
3. coalesce调用导致内存溢出：
因为hdfs中不适合存小文件，所以Spark计算后如果产生的文件太小，我们会调用coalesce合并文件再存入hdfs中。但是这会导致一个问题，例如在coalesce之前有100个文件，这也意味着能够有100个Task，现在调用coalesce(10)，最后只产生10个文件，因为coalesce并不是shuffle操作，这意味着coalesce并不是按照我原本想的那样先执行100个Task，再将Task的执行结果合并成10个，而是从头到位只有10个Task在执行，原本100个文件是分开执行的，现在每个Task同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。解决这个问题的方法是令程序按照我们想的先执行100个Task再将结果合并成10个文件，这个问题同样可以通过repartition解决，调用repartition(10)，因为这就有一个shuffle的过程，shuffle前后是两个Stage，一个100个分区，一个是10个分区，就能按照我们的想法执行。
4. shuffle后内存溢出：
shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在shuffle的使用过程中，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。
5. standalone模式下资源分配不均导致内存溢出：
在standalone的模式下如果配置了--total-executor-cores 和 --executor-memory 这两个参数，但是没有配置--executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。这种情况的解决方法就是同时配置--executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。

----
****

